{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10269322,"sourceType":"datasetVersion","datasetId":6353634}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate torch\n!pip install arabert","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:19:59.221499Z","iopub.execute_input":"2024-12-22T19:19:59.221809Z","iopub.status.idle":"2024-12-22T19:20:05.823713Z","shell.execute_reply.started":"2024-12-22T19:19:59.221785Z","shell.execute_reply":"2024-12-22T19:20:05.822332Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: arabert in /usr/local/lib/python3.10/dist-packages (1.0.1)\nRequirement already satisfied: PyArabic in /usr/local/lib/python3.10/dist-packages (from arabert) (0.6.15)\nRequirement already satisfied: farasapy in /usr/local/lib/python3.10/dist-packages (from arabert) (0.0.14)\nRequirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.10/dist-packages (from arabert) (1.4.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (4.66.5)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic->arabert) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2024.8.30)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nfrom datasets import Dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:05.825248Z","iopub.execute_input":"2024-12-22T19:20:05.825579Z","iopub.status.idle":"2024-12-22T19:20:05.830296Z","shell.execute_reply.started":"2024-12-22T19:20:05.825546Z","shell.execute_reply":"2024-12-22T19:20:05.829601Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset=pd.read_csv('/kaggle/input/aaaaaaaaaaaaaa/all_data_meta.csv',sep=None, engine='python', on_bad_lines='skip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:17.136449Z","iopub.execute_input":"2024-12-22T19:20:17.136784Z","iopub.status.idle":"2024-12-22T19:20:19.231328Z","shell.execute_reply.started":"2024-12-22T19:20:17.136755Z","shell.execute_reply":"2024-12-22T19:20:19.230636Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:20.650718Z","iopub.execute_input":"2024-12-22T19:20:20.651053Z","iopub.status.idle":"2024-12-22T19:20:20.677980Z","shell.execute_reply.started":"2024-12-22T19:20:20.651024Z","shell.execute_reply":"2024-12-22T19:20:20.677205Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       answer_id  question_id  \\\n0         649960       649960   \n1         649961       649961   \n2         649962       649962   \n3         649963       649963   \n4         649959       649959   \n...          ...          ...   \n60646    1191158      1191158   \n60647    1191159      1191159   \n60648    1196592      1196592   \n60649    1196593      1196593   \n60650    1196594      1196594   \n\n                                                    text  answer_start  \\\n0      المعلومات المخزنة أو وسيلة التخزين.\\nأي أن الب...           251   \n1       يتكون البايت عادة من 8 بت، ولذلك فأن البايت ي...           417   \n2      لاحظ أن الأسماء كيلوبايت وميجابايت ...الخ، يمك...           748   \n3      وبتمديد النمط، نستطيع الحصول على وحدتين إضافيت...           882   \n4      البايت أو الثُّمَانِيَّة هي وحدة معلومات رقمية...             0   \n...                                                  ...           ...   \n60646                          ريو دي جانيرو في البرازيل            70   \n60647                                  بوتافوغو ريغاتاس.           104   \n60648   تعود إلى عصور ما قبل التاريخ ونقوش صخرية تعود...           111   \n60649                في الشرق في منطقة قسنطينة (الجزائر)           350   \n60650                                        في عام 1907           512   \n\n       answer_end  answer_category  \\\n0             418              NaN   \n1             536              NaN   \n2             874              NaN   \n3            1015              NaN   \n4             233              NaN   \n...           ...              ...   \n60646          95              NaN   \n60647         121              NaN   \n60648         216              NaN   \n60649         386              NaN   \n60650         523              NaN   \n\n                                                question  file_name  \\\n0                                   اذكر وسيله التخزين ؟  1338943.0   \n1                                    مما يتكون البايت  ؟  1338943.0   \n2                                 ما هي مضاعفات البايت ؟  1338943.0   \n3                               ما هو الاستخدام البايت ؟  1338943.0   \n4                               ما هو البايت ف الحاسوب ؟  1338943.0   \n...                                                  ...        ...   \n60646                                   أين ولد واشنطن ؟  1717957.0   \n60647                 ما هو النادي الذي لعب معه واشنطن ؟  1717957.0   \n60648     متى تعود النقوش الصخرية في منطقة الجلفة إلى؟\\n  1717629.0   \n60649  ما هي المناطق الأخرى في الجزائر التي وُجِدت في...  1717629.0   \n60650                 متى تم اكتشاف موقع زكار؟\\n\\n\\n\\n\\n  1717629.0   \n\n                                             open-domain  \\\n0                                                      1   \n1                                                      1   \n2                                                      1   \n3                                                      1   \n4                                                      1   \n...                                                  ...   \n60646  واشنطن هو لاعب كرة قدم برازيلي في مركز الهجوم،...   \n60647  واشنطن هو لاعب كرة قدم برازيلي في مركز الهجوم،...   \n60648  الفن الصخري بمنطقة الجلفة تتكون النقوش الصخرية...   \n60649  الفن الصخري بمنطقة الجلفة تتكون النقوش الصخرية...   \n60650  الفن الصخري بمنطقة الجلفة تتكون النقوش الصخرية...   \n\n       long                         \\\n0                              1.0   \n1                              1.0   \n2                              1.0   \n3                              1.0   \n4                              1.0   \n...                            ...   \n60646                          1.0   \n60647                          1.0   \n60648                          1.0   \n60649                          0.0   \n60650                          1.0   \n\n                                       Unnamed: 10 Unnamed: 11  \n0             1 long                       0 short        3167  \n1      1 open                        0 no internet         202  \n2                                              NaN         NaN  \n3                                              NaN         NaN  \n4                                              NaN         NaN  \n...                                            ...         ...  \n60646                                            0        None  \n60647                                            0        None  \n60648                                            1        None  \n60649                                            0        None  \n60650                                           0\u001a        None  \n\n[60651 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answer_id</th>\n      <th>question_id</th>\n      <th>text</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n      <th>answer_category</th>\n      <th>question</th>\n      <th>file_name</th>\n      <th>open-domain</th>\n      <th>long</th>\n      <th>Unnamed: 10</th>\n      <th>Unnamed: 11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>649960</td>\n      <td>649960</td>\n      <td>المعلومات المخزنة أو وسيلة التخزين.\\nأي أن الب...</td>\n      <td>251</td>\n      <td>418</td>\n      <td>NaN</td>\n      <td>اذكر وسيله التخزين ؟</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1 long                       0 short</td>\n      <td>3167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>649961</td>\n      <td>649961</td>\n      <td>يتكون البايت عادة من 8 بت، ولذلك فأن البايت ي...</td>\n      <td>417</td>\n      <td>536</td>\n      <td>NaN</td>\n      <td>مما يتكون البايت  ؟</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1 open                        0 no internet</td>\n      <td>202</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>649962</td>\n      <td>649962</td>\n      <td>لاحظ أن الأسماء كيلوبايت وميجابايت ...الخ، يمك...</td>\n      <td>748</td>\n      <td>874</td>\n      <td>NaN</td>\n      <td>ما هي مضاعفات البايت ؟</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>649963</td>\n      <td>649963</td>\n      <td>وبتمديد النمط، نستطيع الحصول على وحدتين إضافيت...</td>\n      <td>882</td>\n      <td>1015</td>\n      <td>NaN</td>\n      <td>ما هو الاستخدام البايت ؟</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>649959</td>\n      <td>649959</td>\n      <td>البايت أو الثُّمَانِيَّة هي وحدة معلومات رقمية...</td>\n      <td>0</td>\n      <td>233</td>\n      <td>NaN</td>\n      <td>ما هو البايت ف الحاسوب ؟</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60646</th>\n      <td>1191158</td>\n      <td>1191158</td>\n      <td>ريو دي جانيرو في البرازيل</td>\n      <td>70</td>\n      <td>95</td>\n      <td>NaN</td>\n      <td>أين ولد واشنطن ؟</td>\n      <td>1717957.0</td>\n      <td>واشنطن هو لاعب كرة قدم برازيلي في مركز الهجوم،...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60647</th>\n      <td>1191159</td>\n      <td>1191159</td>\n      <td>بوتافوغو ريغاتاس.</td>\n      <td>104</td>\n      <td>121</td>\n      <td>NaN</td>\n      <td>ما هو النادي الذي لعب معه واشنطن ؟</td>\n      <td>1717957.0</td>\n      <td>واشنطن هو لاعب كرة قدم برازيلي في مركز الهجوم،...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60648</th>\n      <td>1196592</td>\n      <td>1196592</td>\n      <td>تعود إلى عصور ما قبل التاريخ ونقوش صخرية تعود...</td>\n      <td>111</td>\n      <td>216</td>\n      <td>NaN</td>\n      <td>متى تعود النقوش الصخرية في منطقة الجلفة إلى؟\\n</td>\n      <td>1717629.0</td>\n      <td>الفن الصخري بمنطقة الجلفة تتكون النقوش الصخرية...</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60649</th>\n      <td>1196593</td>\n      <td>1196593</td>\n      <td>في الشرق في منطقة قسنطينة (الجزائر)</td>\n      <td>350</td>\n      <td>386</td>\n      <td>NaN</td>\n      <td>ما هي المناطق الأخرى في الجزائر التي وُجِدت في...</td>\n      <td>1717629.0</td>\n      <td>الفن الصخري بمنطقة الجلفة تتكون النقوش الصخرية...</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60650</th>\n      <td>1196594</td>\n      <td>1196594</td>\n      <td>في عام 1907</td>\n      <td>512</td>\n      <td>523</td>\n      <td>NaN</td>\n      <td>متى تم اكتشاف موقع زكار؟\\n\\n\\n\\n\\n</td>\n      <td>1717629.0</td>\n      <td>الفن الصخري بمنطقة الجلفة تتكون النقوش الصخرية...</td>\n      <td>1.0</td>\n      <td>0\u001a</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>60651 rows × 12 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"pip install torchmetrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:23.072895Z","iopub.execute_input":"2024-12-22T19:20:23.073227Z","iopub.status.idle":"2024-12-22T19:20:26.325880Z","shell.execute_reply.started":"2024-12-22T19:20:23.073169Z","shell.execute_reply":"2024-12-22T19:20:26.324850Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pip install datasets evaluate transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:26.327071Z","iopub.execute_input":"2024-12-22T19:20:26.327342Z","iopub.status.idle":"2024-12-22T19:20:30.150570Z","shell.execute_reply.started":"2024-12-22T19:20:26.327322Z","shell.execute_reply":"2024-12-22T19:20:30.149474Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 1. Fine-tune the target model (see table below) on your training dataset and assess the performance on the test dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom transformers import (\n    GPT2Tokenizer,\n    GPT2LMHeadModel,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    pipeline\n)\nfrom datasets import load_dataset\n\n# Disable WandB (optional)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Load metadata\nmetadata = pd.read_csv(\"/kaggle/input/aaaaaaaaaaaaaa/all_data_meta.csv\", on_bad_lines='skip')\n\n# Combine question and text columns\nmetadata['text'] = metadata['question'].fillna('') + \" \" + metadata['text'].fillna('')\n\n# Save prepared text data\ntext_file_path = \"finetune_data.txt\"\nwith open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n    for line in metadata['text']:\n        f.write(line.strip() + \"\\n\")\n\n# Load AraGPT2 model and tokenizer\nmodel_name = \"aubmindlab/aragpt2-base\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Add special tokens (if necessary)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load the dataset\ndataset = load_dataset('text', data_files={'train': text_file_path})\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\n# Split the dataset into training and evaluation\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Define a data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # No masked language modeling\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name=\"aragpt2-finetuning\",  # Custom run name\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    save_steps=500,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    warmup_steps=500,\n    weight_decay=0.01,\n    fp16=True,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator,  # Use data collator\n)\n\n# Train the model\ntrainer.train()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:31.624491Z","iopub.execute_input":"2024-12-22T19:20:31.624835Z","iopub.status.idle":"2024-12-22T20:02:22.779003Z","shell.execute_reply.started":"2024-12-22T19:20:31.624806Z","shell.execute_reply":"2024-12-22T20:02:22.778335Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05cb591ac8f349dea8e3eb927dd35d0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1baf0b2805d34361a46b23d334400a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97fb5f7f8b42435097b64b1391651718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7902cd2cec84280a6fd6eba4c1e95dd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af7f40434a04db0977c239ad5980d7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e835c8f7da1e4edfbde99039e00e3eec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/108094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a50d6ee648a74564a2c7158f2403dbb2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12161' max='12161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12161/12161 41:18, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.367900</td>\n      <td>3.946282</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12161, training_loss=4.696073472034224, metrics={'train_runtime': 2479.4749, 'train_samples_per_second': 39.236, 'train_steps_per_second': 4.905, 'total_flos': 6354883510272000.0, 'train_loss': 4.696073472034224, 'epoch': 1.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"test model","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n\n# Charger le tokenizer et le modèle\nmodel_path = \"./fine_tuned_aragpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Ajouter des tokens spéciaux si nécessaire\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:12:36.344466Z","iopub.execute_input":"2024-12-22T20:12:36.344786Z","iopub.status.idle":"2024-12-22T20:12:36.519917Z","shell.execute_reply.started":"2024-12-22T20:12:36.344763Z","shell.execute_reply":"2024-12-22T20:12:36.519012Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Préparer une entrée\ninput_text = \"ما هي عاصمة المغرب؟\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Générer une sortie\noutput = model.generate(\n    input_ids,\n    max_length=50,\n    num_return_sequences=1,\n    no_repeat_ngram_size=2,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# Décoder la sortie\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:12:49.581781Z","iopub.execute_input":"2024-12-22T20:12:49.582077Z","iopub.status.idle":"2024-12-22T20:12:51.114911Z","shell.execute_reply.started":"2024-12-22T20:12:49.582054Z","shell.execute_reply":"2024-12-22T20:12:51.114080Z"}},"outputs":[{"name":"stdout","text":"Generated Text: ما هي عاصمة المغرب؟ مدينة مراكش في المغرب. تقع في شمال غرب المغرب على بعد حوالي 70 كم جنوب غرب مدينة الدار البيضاء، عاصمة المملكة المغربية. يقع مقر شركة المغرب للاتصالات السلكية واللاسلكية في مدينة طنجة؛ في جنوب شرق المغرب في منطقة\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Préparer une entrée\ninput_text = \"ما هو الاستخدام البايت ؟\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Générer une sortie\noutput = model.generate(\n    input_ids,\n    max_length=50,\n    num_return_sequences=1,\n    no_repeat_ngram_size=2,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# Décoder la sortie\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:14:30.516686Z","iopub.execute_input":"2024-12-22T20:14:30.516997Z","iopub.status.idle":"2024-12-22T20:14:31.961440Z","shell.execute_reply.started":"2024-12-22T20:14:30.516974Z","shell.execute_reply":"2024-12-22T20:14:31.960565Z"}},"outputs":[{"name":"stdout","text":"Generated Text: ما هو الاستخدام البايت ؟  يستخدم في علاج حالات فرط ضغط الدم في المرضى الذين يعانون من نقص ضغط دم حاد أو نقص في الدم أو فقدان وظائف القلب. يمكن أن يؤدي نقص الدم إلى حدوث اضطراب في القلب أو حدوث ضمور في العضلات أو ضمور العضلات\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Préparer une entrée\ninput_text = \"ين ولد واشنطن ؟\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Générer une sortie\noutput = model.generate(\n    input_ids,\n    max_length=50,\n    num_return_sequences=1,\n    no_repeat_ngram_size=2,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# Décoder la sortie\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:15:46.875752Z","iopub.execute_input":"2024-12-22T20:15:46.876091Z","iopub.status.idle":"2024-12-22T20:15:48.325158Z","shell.execute_reply.started":"2024-12-22T20:15:46.876062Z","shell.execute_reply":"2024-12-22T20:15:48.324279Z"}},"outputs":[{"name":"stdout","text":"Generated Text: ين ولد واشنطن ؟  في الولايات المتحدة. في المملكة المتحدة في عام 1994. ولد في مدينة نيويورك في نيويورك. كان والده في سن الثامنة عشر من عمره. بعد وفاة والده، انتقل إلى الولايات الأمريكية. التحق بمدرسة الفنون الجميلة في جامعة نيويورك�\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"load model","metadata":{}},{"cell_type":"code","source":"# Define the path where you want to save the model and tokenizer\nsave_path = \"./fine_tuned_aragpt2\"\n\n# Save the model and tokenizer\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(f\"Model and tokenizer saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:16.212174Z","iopub.execute_input":"2024-12-22T20:08:16.212539Z","iopub.status.idle":"2024-12-22T20:08:17.632895Z","shell.execute_reply.started":"2024-12-22T20:08:16.212509Z","shell.execute_reply":"2024-12-22T20:08:17.632113Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to ./fine_tuned_aragpt2\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_aragpt2\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_aragpt2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:20.250505Z","iopub.execute_input":"2024-12-22T20:08:20.250819Z","iopub.status.idle":"2024-12-22T20:08:20.434448Z","shell.execute_reply.started":"2024-12-22T20:08:20.250794Z","shell.execute_reply":"2024-12-22T20:08:20.433521Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# 2. Implement a multi-head attention class based on differential attention ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DifferentialAttention(nn.Module):\n    def __init__(self, d_model, num_heads, d_head, lambda_init=0.8):\n        super(DifferentialAttention, self).__init__()\n        assert d_model % (2 * num_heads) == 0, \"d_model must be divisible by 2 * num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_head = d_head\n        self.lambda_init = lambda_init\n\n        # Learnable weights for Q, K, V\n        self.WQ = nn.Parameter(torch.randn(d_model, 2 * d_head))\n        self.WK = nn.Parameter(torch.randn(d_model, 2 * d_head))\n        self.WV = nn.Parameter(torch.randn(d_model, 2 * d_head))\n\n        # Learnable scalars for λ\n        self.lambda_q1 = nn.Parameter(torch.randn(d_head))\n        self.lambda_k1 = nn.Parameter(torch.randn(d_head))\n        self.lambda_q2 = nn.Parameter(torch.randn(d_head))\n        self.lambda_k2 = nn.Parameter(torch.randn(d_head))\n\n    def forward(self, X):\n        N, d_model = X.shape\n\n        # Compute Q1, Q2, K1, K2, V\n        Q = torch.matmul(X, self.WQ)\n        K = torch.matmul(X, self.WK)\n        V = torch.matmul(X, self.WV)\n        Q1, Q2 = Q[:, :self.d_head], Q[:, self.d_head:]\n        K1, K2 = K[:, :self.d_head], K[:, self.d_head:]\n\n        # Compute λ\n        lambda_val = (\n            torch.exp(self.lambda_q1 * self.lambda_k1)\n            - torch.exp(self.lambda_q2 * self.lambda_k2)\n            + self.lambda_init\n        )\n\n        # Differential Attention\n        attn1 = F.softmax(torch.matmul(Q1, K1.T) / (self.d_head ** 0.5), dim=-1)\n        attn2 = F.softmax(torch.matmul(Q2, K2.T) / (self.d_head ** 0.5), dim=-1)\n        diff_attn = attn1 - lambda_val * attn2\n\n        # Weighted sum over values\n        output = torch.matmul(diff_attn, V)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:22.731645Z","iopub.execute_input":"2024-12-22T20:08:22.731937Z","iopub.status.idle":"2024-12-22T20:08:22.740297Z","shell.execute_reply.started":"2024-12-22T20:08:22.731915Z","shell.execute_reply":"2024-12-22T20:08:22.739388Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class MultiHeadDifferentialAttention(nn.Module):\n    def __init__(self, d_model, num_heads, d_head, lambda_init=0.8):\n        super(MultiHeadDifferentialAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.d_head = d_head\n\n        # Create a DifferentialAttention module for each head\n        self.heads = nn.ModuleList([\n            DifferentialAttention(d_model, 1, d_head, lambda_init) for _ in range(num_heads)\n        ])\n\n        # Final linear layer to combine heads\n        self.out_proj = nn.Linear(num_heads * d_head, d_model)\n\n    def forward(self, X):\n        head_outputs = [head(X) for head in self.heads]\n        concatenated = torch.cat(head_outputs, dim=-1)  # Concatenate along feature dimension\n        return self.out_proj(concatenated)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:25.539822Z","iopub.execute_input":"2024-12-22T20:08:25.540113Z","iopub.status.idle":"2024-12-22T20:08:25.545555Z","shell.execute_reply.started":"2024-12-22T20:08:25.540090Z","shell.execute_reply":"2024-12-22T20:08:25.544574Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Replace in a transformer block\nclass CustomTransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_head, lambda_init=0.8):\n        super(CustomTransformerBlock, self).__init__()\n        self.diff_attention = MultiHeadDifferentialAttention(d_model, num_heads, d_head, lambda_init)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model),\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, X):\n        attn_output = self.diff_attention(X)\n        X = self.norm1(X + attn_output)\n        ff_output = self.feed_forward(X)\n        return self.norm2(X + ff_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:27.417736Z","iopub.execute_input":"2024-12-22T20:08:27.418169Z","iopub.status.idle":"2024-12-22T20:08:27.425230Z","shell.execute_reply.started":"2024-12-22T20:08:27.418129Z","shell.execute_reply":"2024-12-22T20:08:27.424216Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from transformers.models.gpt2.modeling_gpt2 import GPT2Model, GPT2Block\nfrom torch.nn import Module\n\nclass CustomGPT2Block(GPT2Block):\n    def __init__(self, config, use_diff_attention=False, diff_attn_params=None):\n        super().__init__(config)\n        self.use_diff_attention = use_diff_attention\n\n        if use_diff_attention:\n            d_model = config.hidden_size\n            num_heads = config.num_attention_heads\n            d_head = d_model // num_heads\n\n            # Initialize the differential attention\n            self.attn = MultiHeadDifferentialAttention(\n                d_model=d_model,\n                num_heads=num_heads,\n                d_head=d_head,\n                lambda_init=diff_attn_params.get(\"lambda_init\", 0.8),\n            )\n\n    def forward(self, hidden_states, layer_past=None, attention_mask=None, use_cache=False):\n        if self.use_diff_attention:\n            attn_output = self.attn(hidden_states)\n        else:\n            attn_output = super().attn(hidden_states, layer_past, attention_mask, use_cache)\n        return super().mlp(attn_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:29.409483Z","iopub.execute_input":"2024-12-22T20:08:29.409822Z","iopub.status.idle":"2024-12-22T20:08:29.415818Z","shell.execute_reply.started":"2024-12-22T20:08:29.409791Z","shell.execute_reply":"2024-12-22T20:08:29.414848Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# 3. Using your fine-tuned model, replace between 25% to 50% of the encoder and/or decoder layers’ multihead attention with multi-head differential attention. For each configuration you propose, train the new model, and assess the performance","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2LMHeadModel\n\ndef modify_gpt2_model(model, replace_fraction=0.25, lambda_init=0.8):\n    \"\"\"\n    Modify the GPT-2 model by replacing a fraction of its attention layers with differential attention.\n    \"\"\"\n    total_layers = len(model.transformer.h)\n    num_layers_to_replace = int(total_layers * replace_fraction)\n    layers_to_replace = sorted(range(total_layers))[-num_layers_to_replace:]  # Replace last layers\n\n    for i in layers_to_replace:\n        model.transformer.h[i] = CustomGPT2Block(\n            config=model.config,\n            use_diff_attention=True,\n            diff_attn_params={\"lambda_init\": lambda_init},\n        )\n    return model\n\n# Load the fine-tuned model and apply modifications\nmodel_name = \"./fine_tuned_aragpt2\"  # Path to fine-tuned model\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\nmodified_model = modify_gpt2_model(model, replace_fraction=0.25)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:09:03.901475Z","iopub.execute_input":"2024-12-22T20:09:03.901819Z","iopub.status.idle":"2024-12-22T20:09:04.253672Z","shell.execute_reply.started":"2024-12-22T20:09:03.901790Z","shell.execute_reply":"2024-12-22T20:09:04.252720Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2LMHeadModel\n\ndef modify_gpt2_model(model, replace_fraction=0.50, lambda_init=0.8):\n    \"\"\"\n    Modify the GPT-2 model by replacing a fraction of its attention layers with differential attention.\n    \"\"\"\n    total_layers = len(model.transformer.h)\n    num_layers_to_replace = int(total_layers * replace_fraction)\n    layers_to_replace = sorted(range(total_layers))[-num_layers_to_replace:]  # Replace last layers\n\n    for i in layers_to_replace:\n        model.transformer.h[i] = CustomGPT2Block(\n            config=model.config,\n            use_diff_attention=True,\n            diff_attn_params={\"lambda_init\": lambda_init},\n        )\n    return model\n\n# Load the fine-tuned model and apply modifications\nmodel_name = \"./fine_tuned_aragpt2\"  # Path to fine-tuned model\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\nmodified_model = modify_gpt2_model(model, replace_fraction=0.50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:09:16.341553Z","iopub.execute_input":"2024-12-22T20:09:16.341883Z","iopub.status.idle":"2024-12-22T20:09:16.974546Z","shell.execute_reply.started":"2024-12-22T20:09:16.341855Z","shell.execute_reply":"2024-12-22T20:09:16.973601Z"}},"outputs":[],"execution_count":30}]}