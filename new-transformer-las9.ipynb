{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10269322,"sourceType":"datasetVersion","datasetId":6353634}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate torch\n!pip install arabert","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:19:59.221499Z","iopub.execute_input":"2024-12-22T19:19:59.221809Z","iopub.status.idle":"2024-12-22T19:20:05.823713Z","shell.execute_reply.started":"2024-12-22T19:19:59.221785Z","shell.execute_reply":"2024-12-22T19:20:05.822332Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: arabert in /usr/local/lib/python3.10/dist-packages (1.0.1)\nRequirement already satisfied: PyArabic in /usr/local/lib/python3.10/dist-packages (from arabert) (0.6.15)\nRequirement already satisfied: farasapy in /usr/local/lib/python3.10/dist-packages (from arabert) (0.0.14)\nRequirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.10/dist-packages (from arabert) (1.4.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (4.66.5)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic->arabert) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2024.8.30)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nfrom datasets import Dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:05.825248Z","iopub.execute_input":"2024-12-22T19:20:05.825579Z","iopub.status.idle":"2024-12-22T19:20:05.830296Z","shell.execute_reply.started":"2024-12-22T19:20:05.825546Z","shell.execute_reply":"2024-12-22T19:20:05.829601Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset=pd.read_csv('/kaggle/input/aaaaaaaaaaaaaa/all_data_meta.csv',sep=None, engine='python', on_bad_lines='skip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:17.136449Z","iopub.execute_input":"2024-12-22T19:20:17.136784Z","iopub.status.idle":"2024-12-22T19:20:19.231328Z","shell.execute_reply.started":"2024-12-22T19:20:17.136755Z","shell.execute_reply":"2024-12-22T19:20:19.230636Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:20.650718Z","iopub.execute_input":"2024-12-22T19:20:20.651053Z","iopub.status.idle":"2024-12-22T19:20:20.677980Z","shell.execute_reply.started":"2024-12-22T19:20:20.651024Z","shell.execute_reply":"2024-12-22T19:20:20.677205Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       answer_id  question_id  \\\n0         649960       649960   \n1         649961       649961   \n2         649962       649962   \n3         649963       649963   \n4         649959       649959   \n...          ...          ...   \n60646    1191158      1191158   \n60647    1191159      1191159   \n60648    1196592      1196592   \n60649    1196593      1196593   \n60650    1196594      1196594   \n\n                                                    text  answer_start  \\\n0      Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ø£Ùˆ ÙˆØ³ÙŠÙ„Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†.\\nØ£ÙŠ Ø£Ù† Ø§Ù„Ø¨...           251   \n1       ÙŠØªÙƒÙˆÙ† Ø§Ù„Ø¨Ø§ÙŠØª Ø¹Ø§Ø¯Ø© Ù…Ù† 8 Ø¨ØªØŒ ÙˆÙ„Ø°Ù„Ùƒ ÙØ£Ù† Ø§Ù„Ø¨Ø§ÙŠØª ÙŠ...           417   \n2      Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙƒÙŠÙ„ÙˆØ¨Ø§ÙŠØª ÙˆÙ…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª ...Ø§Ù„Ø®ØŒ ÙŠÙ…Ùƒ...           748   \n3      ÙˆØ¨ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø·ØŒ Ù†Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙˆØ­Ø¯ØªÙŠÙ† Ø¥Ø¶Ø§ÙÙŠØª...           882   \n4      Ø§Ù„Ø¨Ø§ÙŠØª Ø£Ùˆ Ø§Ù„Ø«ÙÙ‘Ù…ÙØ§Ù†ÙÙŠÙÙ‘Ø© Ù‡ÙŠ ÙˆØ­Ø¯Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø±Ù‚Ù…ÙŠØ©...             0   \n...                                                  ...           ...   \n60646                          Ø±ÙŠÙˆ Ø¯ÙŠ Ø¬Ø§Ù†ÙŠØ±Ùˆ ÙÙŠ Ø§Ù„Ø¨Ø±Ø§Ø²ÙŠÙ„            70   \n60647                                  Ø¨ÙˆØªØ§ÙÙˆØºÙˆ Ø±ÙŠØºØ§ØªØ§Ø³.           104   \n60648   ØªØ¹ÙˆØ¯ Ø¥Ù„Ù‰ Ø¹ØµÙˆØ± Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆÙ†Ù‚ÙˆØ´ ØµØ®Ø±ÙŠØ© ØªØ¹ÙˆØ¯...           111   \n60649                ÙÙŠ Ø§Ù„Ø´Ø±Ù‚ ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ù‚Ø³Ù†Ø·ÙŠÙ†Ø© (Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±)           350   \n60650                                        ÙÙŠ Ø¹Ø§Ù… 1907           512   \n\n       answer_end  answer_category  \\\n0             418              NaN   \n1             536              NaN   \n2             874              NaN   \n3            1015              NaN   \n4             233              NaN   \n...           ...              ...   \n60646          95              NaN   \n60647         121              NaN   \n60648         216              NaN   \n60649         386              NaN   \n60650         523              NaN   \n\n                                                question  file_name  \\\n0                                   Ø§Ø°ÙƒØ± ÙˆØ³ÙŠÙ„Ù‡ Ø§Ù„ØªØ®Ø²ÙŠÙ† ØŸ  1338943.0   \n1                                    Ù…Ù…Ø§ ÙŠØªÙƒÙˆÙ† Ø§Ù„Ø¨Ø§ÙŠØª  ØŸ  1338943.0   \n2                                 Ù…Ø§ Ù‡ÙŠ Ù…Ø¶Ø§Ø¹ÙØ§Øª Ø§Ù„Ø¨Ø§ÙŠØª ØŸ  1338943.0   \n3                               Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø§ÙŠØª ØŸ  1338943.0   \n4                               Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¨Ø§ÙŠØª Ù Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ØŸ  1338943.0   \n...                                                  ...        ...   \n60646                                   Ø£ÙŠÙ† ÙˆÙ„Ø¯ ÙˆØ§Ø´Ù†Ø·Ù† ØŸ  1717957.0   \n60647                 Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø°ÙŠ Ù„Ø¹Ø¨ Ù…Ø¹Ù‡ ÙˆØ§Ø´Ù†Ø·Ù† ØŸ  1717957.0   \n60648     Ù…ØªÙ‰ ØªØ¹ÙˆØ¯ Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ© ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© Ø¥Ù„Ù‰ØŸ\\n  1717629.0   \n60649  Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰ ÙÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ø§Ù„ØªÙŠ ÙˆÙØ¬ÙØ¯Øª ÙÙŠ...  1717629.0   \n60650                 Ù…ØªÙ‰ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…ÙˆÙ‚Ø¹ Ø²ÙƒØ§Ø±ØŸ\\n\\n\\n\\n\\n  1717629.0   \n\n                                             open-domain  \\\n0                                                      1   \n1                                                      1   \n2                                                      1   \n3                                                      1   \n4                                                      1   \n...                                                  ...   \n60646  ÙˆØ§Ø´Ù†Ø·Ù† Ù‡Ùˆ Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø¨Ø±Ø§Ø²ÙŠÙ„ÙŠ ÙÙŠ Ù…Ø±ÙƒØ² Ø§Ù„Ù‡Ø¬ÙˆÙ…ØŒ...   \n60647  ÙˆØ§Ø´Ù†Ø·Ù† Ù‡Ùˆ Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø¨Ø±Ø§Ø²ÙŠÙ„ÙŠ ÙÙŠ Ù…Ø±ÙƒØ² Ø§Ù„Ù‡Ø¬ÙˆÙ…ØŒ...   \n60648  Ø§Ù„ÙÙ† Ø§Ù„ØµØ®Ø±ÙŠ Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© ØªØªÙƒÙˆÙ† Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ©...   \n60649  Ø§Ù„ÙÙ† Ø§Ù„ØµØ®Ø±ÙŠ Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© ØªØªÙƒÙˆÙ† Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ©...   \n60650  Ø§Ù„ÙÙ† Ø§Ù„ØµØ®Ø±ÙŠ Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© ØªØªÙƒÙˆÙ† Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ©...   \n\n       long                         \\\n0                              1.0   \n1                              1.0   \n2                              1.0   \n3                              1.0   \n4                              1.0   \n...                            ...   \n60646                          1.0   \n60647                          1.0   \n60648                          1.0   \n60649                          0.0   \n60650                          1.0   \n\n                                       Unnamed: 10 Unnamed: 11  \n0             1 long                       0 short        3167  \n1      1 open                        0 no internet         202  \n2                                              NaN         NaN  \n3                                              NaN         NaN  \n4                                              NaN         NaN  \n...                                            ...         ...  \n60646                                            0        None  \n60647                                            0        None  \n60648                                            1        None  \n60649                                            0        None  \n60650                                           0\u001a        None  \n\n[60651 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answer_id</th>\n      <th>question_id</th>\n      <th>text</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n      <th>answer_category</th>\n      <th>question</th>\n      <th>file_name</th>\n      <th>open-domain</th>\n      <th>long</th>\n      <th>Unnamed: 10</th>\n      <th>Unnamed: 11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>649960</td>\n      <td>649960</td>\n      <td>Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ø£Ùˆ ÙˆØ³ÙŠÙ„Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†.\\nØ£ÙŠ Ø£Ù† Ø§Ù„Ø¨...</td>\n      <td>251</td>\n      <td>418</td>\n      <td>NaN</td>\n      <td>Ø§Ø°ÙƒØ± ÙˆØ³ÙŠÙ„Ù‡ Ø§Ù„ØªØ®Ø²ÙŠÙ† ØŸ</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1 long                       0 short</td>\n      <td>3167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>649961</td>\n      <td>649961</td>\n      <td>ÙŠØªÙƒÙˆÙ† Ø§Ù„Ø¨Ø§ÙŠØª Ø¹Ø§Ø¯Ø© Ù…Ù† 8 Ø¨ØªØŒ ÙˆÙ„Ø°Ù„Ùƒ ÙØ£Ù† Ø§Ù„Ø¨Ø§ÙŠØª ÙŠ...</td>\n      <td>417</td>\n      <td>536</td>\n      <td>NaN</td>\n      <td>Ù…Ù…Ø§ ÙŠØªÙƒÙˆÙ† Ø§Ù„Ø¨Ø§ÙŠØª  ØŸ</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1 open                        0 no internet</td>\n      <td>202</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>649962</td>\n      <td>649962</td>\n      <td>Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙƒÙŠÙ„ÙˆØ¨Ø§ÙŠØª ÙˆÙ…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª ...Ø§Ù„Ø®ØŒ ÙŠÙ…Ùƒ...</td>\n      <td>748</td>\n      <td>874</td>\n      <td>NaN</td>\n      <td>Ù…Ø§ Ù‡ÙŠ Ù…Ø¶Ø§Ø¹ÙØ§Øª Ø§Ù„Ø¨Ø§ÙŠØª ØŸ</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>649963</td>\n      <td>649963</td>\n      <td>ÙˆØ¨ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø·ØŒ Ù†Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙˆØ­Ø¯ØªÙŠÙ† Ø¥Ø¶Ø§ÙÙŠØª...</td>\n      <td>882</td>\n      <td>1015</td>\n      <td>NaN</td>\n      <td>Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø§ÙŠØª ØŸ</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>649959</td>\n      <td>649959</td>\n      <td>Ø§Ù„Ø¨Ø§ÙŠØª Ø£Ùˆ Ø§Ù„Ø«ÙÙ‘Ù…ÙØ§Ù†ÙÙŠÙÙ‘Ø© Ù‡ÙŠ ÙˆØ­Ø¯Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø±Ù‚Ù…ÙŠØ©...</td>\n      <td>0</td>\n      <td>233</td>\n      <td>NaN</td>\n      <td>Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¨Ø§ÙŠØª Ù Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ØŸ</td>\n      <td>1338943.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60646</th>\n      <td>1191158</td>\n      <td>1191158</td>\n      <td>Ø±ÙŠÙˆ Ø¯ÙŠ Ø¬Ø§Ù†ÙŠØ±Ùˆ ÙÙŠ Ø§Ù„Ø¨Ø±Ø§Ø²ÙŠÙ„</td>\n      <td>70</td>\n      <td>95</td>\n      <td>NaN</td>\n      <td>Ø£ÙŠÙ† ÙˆÙ„Ø¯ ÙˆØ§Ø´Ù†Ø·Ù† ØŸ</td>\n      <td>1717957.0</td>\n      <td>ÙˆØ§Ø´Ù†Ø·Ù† Ù‡Ùˆ Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø¨Ø±Ø§Ø²ÙŠÙ„ÙŠ ÙÙŠ Ù…Ø±ÙƒØ² Ø§Ù„Ù‡Ø¬ÙˆÙ…ØŒ...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60647</th>\n      <td>1191159</td>\n      <td>1191159</td>\n      <td>Ø¨ÙˆØªØ§ÙÙˆØºÙˆ Ø±ÙŠØºØ§ØªØ§Ø³.</td>\n      <td>104</td>\n      <td>121</td>\n      <td>NaN</td>\n      <td>Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø°ÙŠ Ù„Ø¹Ø¨ Ù…Ø¹Ù‡ ÙˆØ§Ø´Ù†Ø·Ù† ØŸ</td>\n      <td>1717957.0</td>\n      <td>ÙˆØ§Ø´Ù†Ø·Ù† Ù‡Ùˆ Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ù‚Ø¯Ù… Ø¨Ø±Ø§Ø²ÙŠÙ„ÙŠ ÙÙŠ Ù…Ø±ÙƒØ² Ø§Ù„Ù‡Ø¬ÙˆÙ…ØŒ...</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60648</th>\n      <td>1196592</td>\n      <td>1196592</td>\n      <td>ØªØ¹ÙˆØ¯ Ø¥Ù„Ù‰ Ø¹ØµÙˆØ± Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆÙ†Ù‚ÙˆØ´ ØµØ®Ø±ÙŠØ© ØªØ¹ÙˆØ¯...</td>\n      <td>111</td>\n      <td>216</td>\n      <td>NaN</td>\n      <td>Ù…ØªÙ‰ ØªØ¹ÙˆØ¯ Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ© ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© Ø¥Ù„Ù‰ØŸ\\n</td>\n      <td>1717629.0</td>\n      <td>Ø§Ù„ÙÙ† Ø§Ù„ØµØ®Ø±ÙŠ Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© ØªØªÙƒÙˆÙ† Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ©...</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60649</th>\n      <td>1196593</td>\n      <td>1196593</td>\n      <td>ÙÙŠ Ø§Ù„Ø´Ø±Ù‚ ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ù‚Ø³Ù†Ø·ÙŠÙ†Ø© (Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±)</td>\n      <td>350</td>\n      <td>386</td>\n      <td>NaN</td>\n      <td>Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰ ÙÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ø§Ù„ØªÙŠ ÙˆÙØ¬ÙØ¯Øª ÙÙŠ...</td>\n      <td>1717629.0</td>\n      <td>Ø§Ù„ÙÙ† Ø§Ù„ØµØ®Ø±ÙŠ Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© ØªØªÙƒÙˆÙ† Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ©...</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>60650</th>\n      <td>1196594</td>\n      <td>1196594</td>\n      <td>ÙÙŠ Ø¹Ø§Ù… 1907</td>\n      <td>512</td>\n      <td>523</td>\n      <td>NaN</td>\n      <td>Ù…ØªÙ‰ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…ÙˆÙ‚Ø¹ Ø²ÙƒØ§Ø±ØŸ\\n\\n\\n\\n\\n</td>\n      <td>1717629.0</td>\n      <td>Ø§Ù„ÙÙ† Ø§Ù„ØµØ®Ø±ÙŠ Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬Ù„ÙØ© ØªØªÙƒÙˆÙ† Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„ØµØ®Ø±ÙŠØ©...</td>\n      <td>1.0</td>\n      <td>0\u001a</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>60651 rows Ã— 12 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"pip install torchmetrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:23.072895Z","iopub.execute_input":"2024-12-22T19:20:23.073227Z","iopub.status.idle":"2024-12-22T19:20:26.325880Z","shell.execute_reply.started":"2024-12-22T19:20:23.073169Z","shell.execute_reply":"2024-12-22T19:20:26.324850Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pip install datasets evaluate transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:26.327071Z","iopub.execute_input":"2024-12-22T19:20:26.327342Z","iopub.status.idle":"2024-12-22T19:20:30.150570Z","shell.execute_reply.started":"2024-12-22T19:20:26.327322Z","shell.execute_reply":"2024-12-22T19:20:30.149474Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 1. Fine-tune the target model (see table below) on your training dataset and assess the performance on the test dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom transformers import (\n    GPT2Tokenizer,\n    GPT2LMHeadModel,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    pipeline\n)\nfrom datasets import load_dataset\n\n# Disable WandB (optional)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Load metadata\nmetadata = pd.read_csv(\"/kaggle/input/aaaaaaaaaaaaaa/all_data_meta.csv\", on_bad_lines='skip')\n\n# Combine question and text columns\nmetadata['text'] = metadata['question'].fillna('') + \" \" + metadata['text'].fillna('')\n\n# Save prepared text data\ntext_file_path = \"finetune_data.txt\"\nwith open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n    for line in metadata['text']:\n        f.write(line.strip() + \"\\n\")\n\n# Load AraGPT2 model and tokenizer\nmodel_name = \"aubmindlab/aragpt2-base\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Add special tokens (if necessary)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load the dataset\ndataset = load_dataset('text', data_files={'train': text_file_path})\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\n# Split the dataset into training and evaluation\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.1)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Define a data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # No masked language modeling\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name=\"aragpt2-finetuning\",  # Custom run name\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    save_steps=500,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    warmup_steps=500,\n    weight_decay=0.01,\n    fp16=True,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator,  # Use data collator\n)\n\n# Train the model\ntrainer.train()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:20:31.624491Z","iopub.execute_input":"2024-12-22T19:20:31.624835Z","iopub.status.idle":"2024-12-22T20:02:22.779003Z","shell.execute_reply.started":"2024-12-22T19:20:31.624806Z","shell.execute_reply":"2024-12-22T20:02:22.778335Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05cb591ac8f349dea8e3eb927dd35d0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1baf0b2805d34361a46b23d334400a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97fb5f7f8b42435097b64b1391651718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7902cd2cec84280a6fd6eba4c1e95dd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af7f40434a04db0977c239ad5980d7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e835c8f7da1e4edfbde99039e00e3eec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/108094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a50d6ee648a74564a2c7158f2403dbb2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12161' max='12161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12161/12161 41:18, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.367900</td>\n      <td>3.946282</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12161, training_loss=4.696073472034224, metrics={'train_runtime': 2479.4749, 'train_samples_per_second': 39.236, 'train_steps_per_second': 4.905, 'total_flos': 6354883510272000.0, 'train_loss': 4.696073472034224, 'epoch': 1.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"test model","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n\n# Charger le tokenizer et le modÃ¨le\nmodel_path = \"./fine_tuned_aragpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Ajouter des tokens spÃ©ciaux si nÃ©cessaire\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:12:36.344466Z","iopub.execute_input":"2024-12-22T20:12:36.344786Z","iopub.status.idle":"2024-12-22T20:12:36.519917Z","shell.execute_reply.started":"2024-12-22T20:12:36.344763Z","shell.execute_reply":"2024-12-22T20:12:36.519012Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# PrÃ©parer une entrÃ©e\ninput_text = \"Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…ØºØ±Ø¨ØŸ\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# GÃ©nÃ©rer une sortie\noutput = model.generate(\n    input_ids,\n    max_length=50,\n    num_return_sequences=1,\n    no_repeat_ngram_size=2,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# DÃ©coder la sortie\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:12:49.581781Z","iopub.execute_input":"2024-12-22T20:12:49.582077Z","iopub.status.idle":"2024-12-22T20:12:51.114911Z","shell.execute_reply.started":"2024-12-22T20:12:49.582054Z","shell.execute_reply":"2024-12-22T20:12:51.114080Z"}},"outputs":[{"name":"stdout","text":"Generated Text: Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…ØºØ±Ø¨ØŸ Ù…Ø¯ÙŠÙ†Ø© Ù…Ø±Ø§ÙƒØ´ ÙÙŠ Ø§Ù„Ù…ØºØ±Ø¨. ØªÙ‚Ø¹ ÙÙŠ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø§Ù„Ù…ØºØ±Ø¨ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¯ Ø­ÙˆØ§Ù„ÙŠ 70 ÙƒÙ… Ø¬Ù†ÙˆØ¨ ØºØ±Ø¨ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¯Ø§Ø± Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ØŒ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©. ÙŠÙ‚Ø¹ Ù…Ù‚Ø± Ø´Ø±ÙƒØ© Ø§Ù„Ù…ØºØ±Ø¨ Ù„Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø³Ù„ÙƒÙŠØ© ÙˆØ§Ù„Ù„Ø§Ø³Ù„ÙƒÙŠØ© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø·Ù†Ø¬Ø©Ø› ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø´Ø±Ù‚ Ø§Ù„Ù…ØºØ±Ø¨ ÙÙŠ Ù…Ù†Ø·Ù‚Ø©\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# PrÃ©parer une entrÃ©e\ninput_text = \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø§ÙŠØª ØŸ\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# GÃ©nÃ©rer une sortie\noutput = model.generate(\n    input_ids,\n    max_length=50,\n    num_return_sequences=1,\n    no_repeat_ngram_size=2,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# DÃ©coder la sortie\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:14:30.516686Z","iopub.execute_input":"2024-12-22T20:14:30.516997Z","iopub.status.idle":"2024-12-22T20:14:31.961440Z","shell.execute_reply.started":"2024-12-22T20:14:30.516974Z","shell.execute_reply":"2024-12-22T20:14:31.960565Z"}},"outputs":[{"name":"stdout","text":"Generated Text: Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø§ÙŠØª ØŸ  ÙŠØ³ØªØ®Ø¯Ù… ÙÙŠ Ø¹Ù„Ø§Ø¬ Ø­Ø§Ù„Ø§Øª ÙØ±Ø· Ø¶ØºØ· Ø§Ù„Ø¯Ù… ÙÙŠ Ø§Ù„Ù…Ø±Ø¶Ù‰ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† Ù†Ù‚Øµ Ø¶ØºØ· Ø¯Ù… Ø­Ø§Ø¯ Ø£Ùˆ Ù†Ù‚Øµ ÙÙŠ Ø§Ù„Ø¯Ù… Ø£Ùˆ ÙÙ‚Ø¯Ø§Ù† ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù‚Ù„Ø¨. ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ¤Ø¯ÙŠ Ù†Ù‚Øµ Ø§Ù„Ø¯Ù… Ø¥Ù„Ù‰ Ø­Ø¯ÙˆØ« Ø§Ø¶Ø·Ø±Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‚Ù„Ø¨ Ø£Ùˆ Ø­Ø¯ÙˆØ« Ø¶Ù…ÙˆØ± ÙÙŠ Ø§Ù„Ø¹Ø¶Ù„Ø§Øª Ø£Ùˆ Ø¶Ù…ÙˆØ± Ø§Ù„Ø¹Ø¶Ù„Ø§Øª\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# PrÃ©parer une entrÃ©e\ninput_text = \"ÙŠÙ† ÙˆÙ„Ø¯ ÙˆØ§Ø´Ù†Ø·Ù† ØŸ\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# GÃ©nÃ©rer une sortie\noutput = model.generate(\n    input_ids,\n    max_length=50,\n    num_return_sequences=1,\n    no_repeat_ngram_size=2,\n    top_k=50,\n    top_p=0.95,\n    temperature=1.0,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# DÃ©coder la sortie\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:15:46.875752Z","iopub.execute_input":"2024-12-22T20:15:46.876091Z","iopub.status.idle":"2024-12-22T20:15:48.325158Z","shell.execute_reply.started":"2024-12-22T20:15:46.876062Z","shell.execute_reply":"2024-12-22T20:15:48.324279Z"}},"outputs":[{"name":"stdout","text":"Generated Text: ÙŠÙ† ÙˆÙ„Ø¯ ÙˆØ§Ø´Ù†Ø·Ù† ØŸ  ÙÙŠ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø©. ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ù…ØªØ­Ø¯Ø© ÙÙŠ Ø¹Ø§Ù… 1994. ÙˆÙ„Ø¯ ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ ÙÙŠ Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ. ÙƒØ§Ù† ÙˆØ§Ù„Ø¯Ù‡ ÙÙŠ Ø³Ù† Ø§Ù„Ø«Ø§Ù…Ù†Ø© Ø¹Ø´Ø± Ù…Ù† Ø¹Ù…Ø±Ù‡. Ø¨Ø¹Ø¯ ÙˆÙØ§Ø© ÙˆØ§Ù„Ø¯Ù‡ØŒ Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©. Ø§Ù„ØªØ­Ù‚ Ø¨Ù…Ø¯Ø±Ø³Ø© Ø§Ù„ÙÙ†ÙˆÙ† Ø§Ù„Ø¬Ù…ÙŠÙ„Ø© ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ù†ÙŠÙˆÙŠÙˆØ±Ùƒï¿½\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"load model","metadata":{}},{"cell_type":"code","source":"# Define the path where you want to save the model and tokenizer\nsave_path = \"./fine_tuned_aragpt2\"\n\n# Save the model and tokenizer\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(f\"Model and tokenizer saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:16.212174Z","iopub.execute_input":"2024-12-22T20:08:16.212539Z","iopub.status.idle":"2024-12-22T20:08:17.632895Z","shell.execute_reply.started":"2024-12-22T20:08:16.212509Z","shell.execute_reply":"2024-12-22T20:08:17.632113Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to ./fine_tuned_aragpt2\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_aragpt2\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_aragpt2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:20.250505Z","iopub.execute_input":"2024-12-22T20:08:20.250819Z","iopub.status.idle":"2024-12-22T20:08:20.434448Z","shell.execute_reply.started":"2024-12-22T20:08:20.250794Z","shell.execute_reply":"2024-12-22T20:08:20.433521Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# 2. Implement a multi-head attention class based on differential attention ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DifferentialAttention(nn.Module):\n    def __init__(self, d_model, num_heads, d_head, lambda_init=0.8):\n        super(DifferentialAttention, self).__init__()\n        assert d_model % (2 * num_heads) == 0, \"d_model must be divisible by 2 * num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_head = d_head\n        self.lambda_init = lambda_init\n\n        # Learnable weights for Q, K, V\n        self.WQ = nn.Parameter(torch.randn(d_model, 2 * d_head))\n        self.WK = nn.Parameter(torch.randn(d_model, 2 * d_head))\n        self.WV = nn.Parameter(torch.randn(d_model, 2 * d_head))\n\n        # Learnable scalars for Î»\n        self.lambda_q1 = nn.Parameter(torch.randn(d_head))\n        self.lambda_k1 = nn.Parameter(torch.randn(d_head))\n        self.lambda_q2 = nn.Parameter(torch.randn(d_head))\n        self.lambda_k2 = nn.Parameter(torch.randn(d_head))\n\n    def forward(self, X):\n        N, d_model = X.shape\n\n        # Compute Q1, Q2, K1, K2, V\n        Q = torch.matmul(X, self.WQ)\n        K = torch.matmul(X, self.WK)\n        V = torch.matmul(X, self.WV)\n        Q1, Q2 = Q[:, :self.d_head], Q[:, self.d_head:]\n        K1, K2 = K[:, :self.d_head], K[:, self.d_head:]\n\n        # Compute Î»\n        lambda_val = (\n            torch.exp(self.lambda_q1 * self.lambda_k1)\n            - torch.exp(self.lambda_q2 * self.lambda_k2)\n            + self.lambda_init\n        )\n\n        # Differential Attention\n        attn1 = F.softmax(torch.matmul(Q1, K1.T) / (self.d_head ** 0.5), dim=-1)\n        attn2 = F.softmax(torch.matmul(Q2, K2.T) / (self.d_head ** 0.5), dim=-1)\n        diff_attn = attn1 - lambda_val * attn2\n\n        # Weighted sum over values\n        output = torch.matmul(diff_attn, V)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:22.731645Z","iopub.execute_input":"2024-12-22T20:08:22.731937Z","iopub.status.idle":"2024-12-22T20:08:22.740297Z","shell.execute_reply.started":"2024-12-22T20:08:22.731915Z","shell.execute_reply":"2024-12-22T20:08:22.739388Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class MultiHeadDifferentialAttention(nn.Module):\n    def __init__(self, d_model, num_heads, d_head, lambda_init=0.8):\n        super(MultiHeadDifferentialAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.d_head = d_head\n\n        # Create a DifferentialAttention module for each head\n        self.heads = nn.ModuleList([\n            DifferentialAttention(d_model, 1, d_head, lambda_init) for _ in range(num_heads)\n        ])\n\n        # Final linear layer to combine heads\n        self.out_proj = nn.Linear(num_heads * d_head, d_model)\n\n    def forward(self, X):\n        head_outputs = [head(X) for head in self.heads]\n        concatenated = torch.cat(head_outputs, dim=-1)  # Concatenate along feature dimension\n        return self.out_proj(concatenated)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:25.539822Z","iopub.execute_input":"2024-12-22T20:08:25.540113Z","iopub.status.idle":"2024-12-22T20:08:25.545555Z","shell.execute_reply.started":"2024-12-22T20:08:25.540090Z","shell.execute_reply":"2024-12-22T20:08:25.544574Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Replace in a transformer block\nclass CustomTransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_head, lambda_init=0.8):\n        super(CustomTransformerBlock, self).__init__()\n        self.diff_attention = MultiHeadDifferentialAttention(d_model, num_heads, d_head, lambda_init)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model),\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, X):\n        attn_output = self.diff_attention(X)\n        X = self.norm1(X + attn_output)\n        ff_output = self.feed_forward(X)\n        return self.norm2(X + ff_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:27.417736Z","iopub.execute_input":"2024-12-22T20:08:27.418169Z","iopub.status.idle":"2024-12-22T20:08:27.425230Z","shell.execute_reply.started":"2024-12-22T20:08:27.418129Z","shell.execute_reply":"2024-12-22T20:08:27.424216Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from transformers.models.gpt2.modeling_gpt2 import GPT2Model, GPT2Block\nfrom torch.nn import Module\n\nclass CustomGPT2Block(GPT2Block):\n    def __init__(self, config, use_diff_attention=False, diff_attn_params=None):\n        super().__init__(config)\n        self.use_diff_attention = use_diff_attention\n\n        if use_diff_attention:\n            d_model = config.hidden_size\n            num_heads = config.num_attention_heads\n            d_head = d_model // num_heads\n\n            # Initialize the differential attention\n            self.attn = MultiHeadDifferentialAttention(\n                d_model=d_model,\n                num_heads=num_heads,\n                d_head=d_head,\n                lambda_init=diff_attn_params.get(\"lambda_init\", 0.8),\n            )\n\n    def forward(self, hidden_states, layer_past=None, attention_mask=None, use_cache=False):\n        if self.use_diff_attention:\n            attn_output = self.attn(hidden_states)\n        else:\n            attn_output = super().attn(hidden_states, layer_past, attention_mask, use_cache)\n        return super().mlp(attn_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:08:29.409483Z","iopub.execute_input":"2024-12-22T20:08:29.409822Z","iopub.status.idle":"2024-12-22T20:08:29.415818Z","shell.execute_reply.started":"2024-12-22T20:08:29.409791Z","shell.execute_reply":"2024-12-22T20:08:29.414848Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# 3. Using your fine-tuned model, replace between 25% to 50% of the encoder and/or decoder layersâ€™ multihead attention with multi-head differential attention. For each configuration you propose, train the new model, and assess the performance","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2LMHeadModel\n\ndef modify_gpt2_model(model, replace_fraction=0.25, lambda_init=0.8):\n    \"\"\"\n    Modify the GPT-2 model by replacing a fraction of its attention layers with differential attention.\n    \"\"\"\n    total_layers = len(model.transformer.h)\n    num_layers_to_replace = int(total_layers * replace_fraction)\n    layers_to_replace = sorted(range(total_layers))[-num_layers_to_replace:]  # Replace last layers\n\n    for i in layers_to_replace:\n        model.transformer.h[i] = CustomGPT2Block(\n            config=model.config,\n            use_diff_attention=True,\n            diff_attn_params={\"lambda_init\": lambda_init},\n        )\n    return model\n\n# Load the fine-tuned model and apply modifications\nmodel_name = \"./fine_tuned_aragpt2\"  # Path to fine-tuned model\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\nmodified_model = modify_gpt2_model(model, replace_fraction=0.25)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:09:03.901475Z","iopub.execute_input":"2024-12-22T20:09:03.901819Z","iopub.status.idle":"2024-12-22T20:09:04.253672Z","shell.execute_reply.started":"2024-12-22T20:09:03.901790Z","shell.execute_reply":"2024-12-22T20:09:04.252720Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from transformers import GPT2Config, GPT2LMHeadModel\n\ndef modify_gpt2_model(model, replace_fraction=0.50, lambda_init=0.8):\n    \"\"\"\n    Modify the GPT-2 model by replacing a fraction of its attention layers with differential attention.\n    \"\"\"\n    total_layers = len(model.transformer.h)\n    num_layers_to_replace = int(total_layers * replace_fraction)\n    layers_to_replace = sorted(range(total_layers))[-num_layers_to_replace:]  # Replace last layers\n\n    for i in layers_to_replace:\n        model.transformer.h[i] = CustomGPT2Block(\n            config=model.config,\n            use_diff_attention=True,\n            diff_attn_params={\"lambda_init\": lambda_init},\n        )\n    return model\n\n# Load the fine-tuned model and apply modifications\nmodel_name = \"./fine_tuned_aragpt2\"  # Path to fine-tuned model\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\nmodified_model = modify_gpt2_model(model, replace_fraction=0.50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:09:16.341553Z","iopub.execute_input":"2024-12-22T20:09:16.341883Z","iopub.status.idle":"2024-12-22T20:09:16.974546Z","shell.execute_reply.started":"2024-12-22T20:09:16.341855Z","shell.execute_reply":"2024-12-22T20:09:16.973601Z"}},"outputs":[],"execution_count":30}]}